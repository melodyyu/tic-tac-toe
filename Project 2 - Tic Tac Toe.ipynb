{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tic Tac Toe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u> Description <u> \n",
    "In this implementation of a 4x4 tic-tac-toe game, we will use two reinforcement learning (RL) algorithms: Monte Carlo (MC) and Q-Learning. We begin by describing the major components.\n",
    "\n",
    "- **Agent** <br>\n",
    "Player 1 and player 2.\n",
    "\n",
    "- **Environment** <br> \n",
    "The board will be initialized as a 4x4 grid containing only zeroes. When player places their piece, the position will be updated with 1 if the move came from player 1 and -1 if the move came from player 2. \n",
    "\n",
    "- **State** <br>\n",
    "The board state (current piece placements and available spaces) of the agent and its opponent. \n",
    "\n",
    "- **Actions** <br>\n",
    "The positions that a player can choose based on the current board state. At each position, players can either play a piece or cannot (the piece is in use by the opponent). Players will take turns placing pieces and will continue until terminal state is reached. The position they place a piece will be randomly selected from the open positions.\n",
    "\n",
    "- **Terminal state** <br>\n",
    "Players cannot move anymore (the board is filled and/or a win/lose/draw condition has been reached). \n",
    "\n",
    "- **Reward** <br>\n",
    "The player receives +1 reward if they win, -1 reward if they lose and 0 reward if they draw. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>Environment<u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import random "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOARD_ROWS = 4\n",
    "BOARD_COLS = 4\n",
    "\n",
    "PLAYER_X = 1 \n",
    "PLAYER_0 = 0 \n",
    "\n",
    "GAME_STATE_X = -1\n",
    "GAME_STATE_O = 1\n",
    "GAME_STATE_DRAW = 0\n",
    "GAME_STATE_NOT_ENDED = 2\n",
    "\n",
    "states = [] \n",
    "\n",
    "##initialize board \n",
    "class Environment:\n",
    "    def __init__ (self):\n",
    "        self.board = np.zeros((BOARD_ROWS, BOARD_COLS))\n",
    "        self.terminal = False #bool - game signals terminal state\n",
    "        self.playerSymbol = PLAYER_X\n",
    "         \n",
    "    #returns position at specific location \n",
    "    def get_position (self, x, y):\n",
    "        return self.board[x][y] #note: [x][y] == [x,y]\n",
    "    \n",
    "    #set position (i.e, update state) at specific index to player's symbol\n",
    "    def set_position (self, x, y, player_symbol):\n",
    "        self.board[x][y] = player_symbol\n",
    "        \n",
    "#         if ((x > 4) or (y > 4) or (x < 0) or (y < 0)): \n",
    "#             print(\"you're outta bounds sir\")\n",
    "        \n",
    "    def print_board(self):\n",
    "        print(self.board)\n",
    "        \n",
    "    #save state to array; \n",
    "#     def store_state(self):\n",
    "        \n",
    "        \n",
    "    #return if position is open \n",
    "    def check_open(self, x, y):\n",
    "        #position was empty, return T;\n",
    "        if (self.board[x][y] == 0): \n",
    "            return True \n",
    "        else: \n",
    "            #position was filled, return F \n",
    "            return False \n",
    "    \n",
    "    #determine winner; if agent wins, return 1. if opponent wins, return -1. \n",
    "    def winner(self):\n",
    "        \n",
    "        winner = None \n",
    "\n",
    "        #horizontal win -- player gets 4 in a row across\n",
    "        for i in range(BOARD_ROWS):\n",
    "            if sum(self.board[i, :]) == 4:\n",
    "                self.terminal = True\n",
    "                winner = 1\n",
    "            if sum(self.board[i, :]) == 0:\n",
    "                self.terminal = True\n",
    "                winner = -1\n",
    "\n",
    "        # vertical win -- player gets 4 in a column \n",
    "        for i in range(BOARD_COLS):\n",
    "            if sum(self.board[:, i]) == 4:\n",
    "                self.terminal = True\n",
    "                winner = 1\n",
    "            if sum(self.board[:, i]) == 0:\n",
    "                self.terminal = True\n",
    "                winner = -1\n",
    "        \n",
    "        # diagonal win \n",
    "        diag_sum1 = sum([self.board[i, i] for i in range(BOARD_COLS)])\n",
    "        diag_sum2 = sum([self.board[i, BOARD_COLS-i-1] for i in range(BOARD_COLS)])\n",
    "        diag_sum = max(diag_sum1, diag_sum2)\n",
    "        if diag_sum == 4:\n",
    "            self.terminal = True\n",
    "            winner = 1\n",
    "        if diag_sum == 0:\n",
    "            self.terminal = True\n",
    "            winner = -1\n",
    "        \n",
    "        # tie -- no more available positions\n",
    "        if len(self.open_positions()) == 0:\n",
    "            self.terminal = True\n",
    "            winner = 0\n",
    "      \n",
    "        # if the game has not ended, simply return nothing \n",
    "        self.terminal = False\n",
    "        print(\"Reward is: \", winner)\n",
    "        return winner\n",
    "            \n",
    "\n",
    "#     #determine reward based on winner\n",
    "#     def reward(self): \n",
    "#         result = self.winner() \n",
    "        \n",
    "#         #agent won\n",
    "#         if (result == 1): \n",
    "            \n",
    "#         #opponent won\n",
    "#         if (result == -1):\n",
    "            \n",
    "#         #tie -- no reward \n",
    "            \n",
    "    \n",
    "    #return an array of open positions in the board\n",
    "    def open_positions(self): \n",
    "        positions = [] \n",
    "        for x in range(BOARD_ROWS):\n",
    "            for y in range(BOARD_COLS):\n",
    "                if self.board[x,y] == 0:\n",
    "                    positions.append((x,y))\n",
    "        return positions\n",
    "        \n",
    "    #clear board, reset all positions to 0\n",
    "    def reset(self): \n",
    "        self.board = np.zeros((BOARD_ROWS, BOARD_COLS))\n",
    "        self.terminal = False\n",
    "        self.playerSymbol = AGENT_SYMBOL\n",
    "    \n",
    "    #print board\n",
    "    def show_board(self):\n",
    "        # p1: x  p2: o\n",
    "        for i in range(0, BOARD_ROWS):\n",
    "            print('-------------')\n",
    "            out = '| '\n",
    "            for j in range(0, BOARD_COLS):\n",
    "                if self.board[i, j] == 1:\n",
    "                    token = 'x'\n",
    "                if self.board[i, j] == -1:\n",
    "                    token = 'o'\n",
    "                if self.board[i, j] == 0:\n",
    "                    token = ' '\n",
    "                out += token + ' | '\n",
    "            print(out)\n",
    "        print('-------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Player:\n",
    "#     def __init__ (self):\n",
    "#         self.states = [] \n",
    "#         self.env = Environment() \n",
    "        \n",
    "#     def chooseAction(self, positions, current_board, symbol): \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "import os\n",
    "import pickle\n",
    "import collections\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "class Learner(ABC):\n",
    "    \"\"\"\n",
    "    Parent class for Q-learning and SARSA agents.\n",
    "    Parameters\n",
    "    ----------\n",
    "    alpha : float \n",
    "        learning rate\n",
    "    gamma : float\n",
    "        temporal discounting rate\n",
    "    eps : float \n",
    "        probability of random action vs. greedy action\n",
    "    eps_decay : float\n",
    "        epsilon decay rate. Larger value = more decay\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha, gamma, eps, eps_decay=0.):\n",
    "        # Agent parameters\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.eps = eps\n",
    "        self.eps_decay = eps_decay\n",
    "        # Possible actions correspond to the set of all x,y coordinate pairs\n",
    "        self.actions = []\n",
    "        for i in range(3):\n",
    "            for j in range(3):\n",
    "                self.actions.append((i,j))\n",
    "        # Initialize Q values to 0 for all state-action pairs.\n",
    "        # Access value for action a, state s via Q[a][s]\n",
    "        self.Q = {}\n",
    "        for action in self.actions:\n",
    "            self.Q[action] = collections.defaultdict(int)\n",
    "        # Keep a list of reward received at each episode\n",
    "        self.rewards = []\n",
    "\n",
    "    def get_action(self, s):\n",
    "        \"\"\"\n",
    "        Select an action given the current game state.\n",
    "        Parameters\n",
    "        ----------\n",
    "        s : string\n",
    "            state\n",
    "        \"\"\"\n",
    "        # Only consider the allowed actions (empty board spaces)\n",
    "        possible_actions = [a for a in self.actions if s[a[0]*3 + a[1]] == '-']\n",
    "        if random.random() < self.eps:\n",
    "            # Random choose.\n",
    "            action = possible_actions[random.randint(0,len(possible_actions)-1)]\n",
    "        else:\n",
    "            # Greedy choose.\n",
    "            values = np.array([self.Q[a][s] for a in possible_actions])\n",
    "            # Find location of max\n",
    "            ix_max = np.where(values == np.max(values))[0]\n",
    "            if len(ix_max) > 1:\n",
    "                # If multiple actions were max, then sample from them\n",
    "                ix_select = np.random.choice(ix_max, 1)[0]\n",
    "            else:\n",
    "                # If unique max action, select that one\n",
    "                ix_select = ix_max[0]\n",
    "            action = possible_actions[ix_select]\n",
    "\n",
    "        # update epsilon; geometric decay\n",
    "        self.eps *= (1.-self.eps_decay)\n",
    "\n",
    "        return action\n",
    "\n",
    "    def save_agent(self, path):\n",
    "        \"\"\" Pickle the agent object instance to save the agent's state. \"\"\"\n",
    "        if os.path.isfile(path):\n",
    "            os.remove(path)\n",
    "        f = open(path, 'wb')\n",
    "        pickle.dump(self, f)\n",
    "        f.close()\n",
    "\n",
    "    @abstractmethod\n",
    "    def update(self, s, s_, a, a_, r):\n",
    "        pass\n",
    "\n",
    "\n",
    "class Qlearner(Learner):\n",
    "    \"\"\"\n",
    "    A class to implement the Q-learning agent.\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha, gamma, eps, eps_decay=0.):\n",
    "        super().__init__(alpha, gamma, eps, eps_decay)\n",
    "\n",
    "    def update(self, s, s_, a, a_, r):\n",
    "        \"\"\"\n",
    "        Perform the Q-Learning update of Q values.\n",
    "        Parameters\n",
    "        ----------\n",
    "        s : string\n",
    "            previous state\n",
    "        s_ : string\n",
    "            new state\n",
    "        a : (i,j) tuple\n",
    "            previous action\n",
    "        a_ : (i,j) tuple\n",
    "            new action. NOT used by Q-learner!\n",
    "        r : int\n",
    "            reward received after executing action \"a\" in state \"s\"\n",
    "        \"\"\"\n",
    "        # Update Q(s,a)\n",
    "        if s_ is not None:\n",
    "            # hold list of Q values for all a_,s_ pairs. We will access the max later\n",
    "            possible_actions = [action for action in self.actions if s_[action[0]*3 + action[1]] == '-']\n",
    "            Q_options = [self.Q[action][s_] for action in possible_actions]\n",
    "            # update\n",
    "            self.Q[a][s] += self.alpha*(r + self.gamma*max(Q_options) - self.Q[a][s])\n",
    "        else:\n",
    "            # terminal state update\n",
    "            self.Q[a][s] += self.alpha*(r - self.Q[a][s])\n",
    "\n",
    "        # add r to rewards list\n",
    "        self.rewards.append(r)\n",
    "\n",
    "\n",
    "class SARSAlearner(Learner):\n",
    "    \"\"\"\n",
    "    A class to implement the SARSA agent.\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha, gamma, eps, eps_decay=0.):\n",
    "        super().__init__(alpha, gamma, eps, eps_decay)\n",
    "\n",
    "    def update(self, s, s_, a, a_, r):\n",
    "        \"\"\"\n",
    "        Perform the SARSA update of Q values.\n",
    "        Parameters\n",
    "        ----------\n",
    "        s : string\n",
    "            previous state\n",
    "        s_ : string\n",
    "            new state\n",
    "        a : (i,j) tuple\n",
    "            previous action\n",
    "        a_ : (i,j) tuple\n",
    "            new action\n",
    "        r : int\n",
    "            reward received after executing action \"a\" in state \"s\"\n",
    "        \"\"\"\n",
    "        # Update Q(s,a)\n",
    "        if s_ is not None:\n",
    "            self.Q[a][s] += self.alpha*(r + self.gamma*self.Q[a_][s_] - self.Q[a][s])\n",
    "        else:\n",
    "            # terminal state update\n",
    "            self.Q[a][s] += self.alpha*(r - self.Q[a][s])\n",
    "\n",
    "        # add r to rewards list\n",
    "        self.rewards.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = Environment()\n",
    "test.board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(test.check_open(2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 4 is out of bounds for axis 0 with size 4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-363bf6684834>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_position\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboard\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-44f232c3a294>\u001b[0m in \u001b[0;36mset_position\u001b[0;34m(self, x, y, player_symbol)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m#set position (i.e, update state) at specific index to player's symbol\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_position\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplayer_symbol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboard\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplayer_symbol\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprint_board\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 4 is out of bounds for axis 0 with size 4"
     ]
    }
   ],
   "source": [
    "test.set_position(2,4,2)\n",
    "test.board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Player: \n",
    "    def __init__ (self, player): \n",
    "#         self.player = p1 \n",
    "        self.player_symbol = player #player's symbol is either -1 or 1\n",
    "        \n",
    "    def place (): \n",
    "        probability = random.randint(0,1) \n",
    "        if (probability > 0.5): \n",
    "            return True\n",
    "        else:\n",
    "            return False "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
